\documentclass[10pt,a4j]{ujarticle}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings,jlisting}
\usepackage{ascmac}
\usepackage{amsmath,amssymb}

ここからソースコードの表示に関する設定
\lstset{
  basicstyle={\ttfamily},
  identifierstyle={\small},
  commentstyle={\smallitshape},
  keywordstyle={\small\bfseries},
  ndkeywordstyle={\small},
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},
  numbers=left,
  xrightmargin=0zw,
  xleftmargin=3zw,
  numberstyle={\scriptsize},
  stepnumber=1,
  numbersep=1zw,
  lineskip=-0.5ex
}
ここまでソースコードの表示に関する設定

\title{知能処理学 レポート}
\author{
 36714029 遠藤裕人\\
  {\small (学生番号と氏名が必要)}
}
\date{\today}


\begin{document}
\maketitle

\paragraph{提出レポート: } rep0
\paragraph{全体的な自己評価／作業時間: } S／6時間

自己評価を S, A, B, C, D から選択する．作業時間は成績に影響しないので正直に書くこと．

\paragraph{評価の理由: } 迷路探索アルゴリズムの実装において各処理（ゴール検査選択展開生成）を独立したメソッドに分離することで可読性と保守性を向上させた．また幅優先探索と深さ優先探索の違いを引数の順序変更のみで実現できることを示しアルゴリズムの本質的な理解を深めることができたため．



\section{課題2-1e 全体的な考察}

\section{課題2-1a}

\paragraph{自己評価／作業時間: } S 2時間23分

\section{課題1-1}

\begin{screen}
  講義で示した min-max 法や α-β カット法の擬似コードと見比べながらレポートにてその実現方法を説明する
\end{screen}

\subsection{min-max 法} 
\begin{lstlisting}
float maxSearch(State state, int depth) {
    if (isTerminal(state, depth))
        return this.eval.value(state);
    var v = NEGATIVE_INFINITY;
    for (var move: state.getMoves()) {
        var next = state.perform(move);
        var v0 = minSearch(next, depth + 1);
        v = Math.max(v, v0);
    }
    return v;
}
\end{lstlisting}
一つ下のノードに対してminSearchを呼び出している。できるだけ低い評価値の中から最大値を選んでいる。
\begin{lstlisting}
float minSearch(State state, int depth) {
    if (isTerminal(state, depth))
        return this.eval.value(state);
    var v = POSITIVE_INFINITY;
    for (var move: state.getMoves()) {
            var next = state.perform(move);
            var v0 = maxSearch(next, depth + 1);
            v = Math.min(v, v0);
    }
    return v;
}
\end{lstlisting}
一つ下のノードに対してmaxSearchを呼び出している。相手が最良の手を選ぶ前提である。


\subsection{α-β カット法} 
\begin{lstlisting}
float maxSearch(State state, float alpha, float beta, int depth) {
  //葉ノード（探索の末端）の評価値
    if (isTerminal(state, depth)) {
        float val = this.eval.value(state);
        System.out.println("  ".repeat(depth) + "Leaf " + state + " = " + val);
        return val;
    }
    float v = NEGATIVE_INFINITY;
    System.out.println("  ".repeat(depth) + "MAX at " + state + " [alpha=" + alpha + ", beta=" + beta + "]");
    for (var move: state.getMoves()) {
        var next = state.perform(move);
        float v0 = minSearch(next, alpha, beta, depth + 1);
        v = Math.max(v, v0);
        if (beta <= v0) {
            System.out.println("  ".repeat(depth) + "PRUNED (beta cutoff) at " + state);
            break;
        }
        alpha = Math.max(alpha, v0);
    }
    return v;
}
\end{lstlisting}
親ノードより子ノードの評価値が良ければ探索を打ち切る。これがβカットである。実質的に親ノードと一つ下の子ノードの評価値だけを観察しており、それ以下のノードの評価値はminSearch内にある。
\begin{lstlisting}
float minSearch(State state, float alpha, float beta, int depth) {
    if (isTerminal(state, depth)) {
        float val = this.eval.value(state);
        System.out.println("  ".repeat(depth) + "Leaf " + state + " = " + val);
        return val;
    }
    float v = POSITIVE_INFINITY;
    System.out.println("  ".repeat(depth) + "MIN at " + state + " [alpha=" + alpha + ", beta=" + beta + "]");
    for (var move: state.getMoves()) {
        var next = state.perform(move);
        float v0 = maxSearch(next, alpha, beta, depth + 1);
        v = Math.min(v, v0);
        if (alpha >= v0) {
            System.out.println("  ".repeat(depth) + "PRUNED (alpha cutoff) at " + state + " after evaluating " + next);
            break;
        }
        beta = Math.min(beta, v0);
    }
    return v;
}
\end{lstlisting}
親ノードより子ノードの評価値が悪ければ探索を打ち切る。これがαカットである。

\begin{screen}
  リスト3を利用して図2の評価値を求めよ
\end{screen}

MAX at Root [alpha=-Infinity, beta=Infinity]
  MIN at L1 [alpha=-Infinity, beta=Infinity]
    Leaf LL1 = -1.0
    Leaf LL2 = -31.0
    Leaf LL3 = -16.0
  MIN at L2 [alpha=-31.0, beta=Infinity]
    Leaf ML1 = -38.0
  PRUNED (alpha cutoff) at L2 after evaluating ML1
  MIN at L3 [alpha=-31.0, beta=Infinity]
    Leaf RL1 = -9.0
    Leaf RL2 = 6.0
AlphaBeta evaluation: -9.0

よって評価値は-9.0となる。

またML1の評価値-38.0はL2ノードの評価値-31.0より小さいためαカットが発生しRL1 RL2の評価値は計算されていない。


\subsection{考察・感想}

\paragraph{考察: }

\paragraph{感想: }

\section{課題2-1b}

\paragraph{自己評価／作業時間: } S 2時間00分

\section{課題2-1}

\begin{screen}
  講義で示したアルゴリズムと比較すること．各クラスの役割をレポートにて説明すること
\end{screen}


Evalクラス
状態の評価値を計算する役割を持つ。valueメソッドはStateオブジェクトを受け取り、その状態がゴール状態であれば無限大の評価値を返し、そうでなければ石の数に基づいて評価値を計算する。

Playerクラス
ゲームプレイヤーの抽象クラス。nextMoveメソッドはStateオブジェクトを受け取り、次に取るべき手を決定して返す。このクラスを継承して具体的な戦略を実装する。
また、色や名前を保持するためのフィールドとコンストラクタも含まれている。

RandomPlayerクラス
Playerクラスを継承し、ランダムに石を取り除く戦略を実装する。nextMoveメソッドはStateオブジェクトを受け取り、可能な手の中からランダムに一つを選択して返す。

Stateクラス
ゲームの状態を表現する役割を持つ。現在の石の数、現在のプレイヤー、各プレイヤーが獲得した石の数などの情報を保持する。getMovesメソッドは現在の状態から可能な手をリストとして返し、performメソッドは指定された手を実行した後の新しい状態を返す。

\begin{screen}
  石の個数を変えたり，先行後攻を入れ替えたりして RandomPlayer と MinMaxPlayer を対戦させた結果を報告せよ
\end{screen}
石の数が5のとき、MinMaxPlayerが3つの石を獲得したため勝利した。
==== 5 stone(s) ====
 5 ->  4 | Random(o) took 1 stone(s).
 4 ->  1 | MinMax20(x) took 3 stone(s).
 1 ->  0 | Random(o) took 1 stone(s).
Winner: MinMax20(x)

石の数が20のとき、MinMaxPlayerが11つの石を獲得したため勝利した。
==== 20 stone(s) ====
20 -> 18 | Random(o) took 2 stone(s).
18 -> 17 | MinMax20(x) took 1 stone(s).
17 -> 14 | Random(o) took 3 stone(s).
14 -> 13 | MinMax20(x) took 1 stone(s).
13 -> 12 | Random(o) took 1 stone(s).
12 ->  9 | MinMax20(x) took 3 stone(s).
 9 ->  8 | Random(o) took 1 stone(s).
 8 ->  5 | MinMax20(x) took 3 stone(s).
 5 ->  4 | Random(o) took 1 stone(s).
 4 ->  1 | MinMax20(x) took 3 stone(s).
 1 ->  0 | Random(o) took 1 stone(s).
Winner: MinMax20(x)
\section{課題2-1c}
\paragraph{自己評価／作業時間: } A 1時間00分

\section{課題2-1}

\begin{screen}
課題 2-1b で作成した MinMaxPlayer を改変して，α-β カット法を実装せよ
\end{screen}

=== 探索統計情報 ===
訪問ノード数: 39
枝刈り回数: 8
  αカット: 2
  βカット: 6

[βカット] 枝刈り発生
箇所:  3
理由: v(-1000.0) >= β(-1000.0)

\section{課題2-1d}
\paragraph{自己評価／作業時間: } S 3時間16分

\section{課題2-1}

\begin{screen}
d が無制限の場合に比べて MinMaxPlayer の強さがどのように変化したのか報告せよ．
\end{screen}
N=20より
==== 20 stone(s) ====
20 -> 17 | Random(o) took 3 stone(s).
17 -> 14 | MinMax20(x) took 3 stone(s).
14 -> 11 | Random(o) took 3 stone(s).
11 ->  9 | MinMax20(x) took 2 stone(s).
 9 ->  6 | Random(o) took 3 stone(s).
 6 ->  5 | MinMax20(x) took 1 stone(s).
 5 ->  4 | Random(o) took 1 stone(s).
 4 ->  1 | MinMax20(x) took 3 stone(s).
 1 ->  0 | Random(o) took 1 stone(s).
Winner: MinMax20(x)

N=3より
==== 3 stone(s) ====
 3 ->  0 | Random(o) took 3 stone(s).
Winner: MinMax20(x)

N=2より
==== 2 stone(s) ====
 2 ->  1 | Random(o) took 1 stone(s).
 1 ->  0 | MinMax20(x) took 1 stone(s).
Winner: Random(o)

\section{課題2-2}

\begin{screen}
  なぜそのように変化したのかをEval.javaのコードとmin-max法の仕組みに基づき論理的に説明せよ
\end{screen}
public class Eval {
    float value(State state) {
      float s = state.winner().getSign(); //  +1, -1, 0 
      return state.isGoal() ? Float.POSITIVE_INFINITY * s : s / state.numStones; 
    }
}
残り石数が少ないほど評価値の絶対値が大きくなるため、終盤になるほど評価値の差が大きくなる。
そのため、探索深さが制限されている場合でも、終盤では正確な評価が可能となりMinMaxPlayerの強さが維持される。一方、序盤では評価値の差が小さいため、浅い探索深さでは最適な手を選択できず、MinMaxPlayerの強さが低下する。

\section{課題2-3}

\begin{screen}
  深さが浅くてもある程度賢く振る舞うためには、Eval クラスをどのように改良すべきかを考察し，具体的なアイデア（ヒューリスティック）を提案せよ
\end{screen}
\begin{lstlisting}
float value(State state) {
if (state.isGoal()) {
        float s = state.winner().getSign();
        return Float.POSITIVE_INFINITY * s;
    }
    float s = state.winner().getSign();
    int mod4 = state.numStones % 4;
    
    // 4の倍数を相手に残せている = 有利
    // 4の倍数が自分のターン = 不利
    float modBonus = (mod4 == 0) ? -1.0f : 1.0f;
    
    // 終盤ほど評価値を大きく
    float endgameWeight = 1.0f / (state.numStones + 1);
    
    return s * (modBonus * 10.0f + endgameWeight);
}
\end{lstlisting}
自分のターン終了時に相手に4の倍数の石を残すことができれば有利であるため、その状況を評価値に反映する。また、終盤になるほど評価値の差が重要になるため、残り石数に応じて評価値を調整する。
\section{課題2-4}

\begin{screen}
上のアイデアを実際に実装し，ランダムプレイヤーとの勝率の変化を検証せよ（常勝は NG）提出先ディレクトリを rep21d とする．
\end{screen}
ヒューリスティック関数適用前
勝った回数：19回
負けた回数：1回
ヒューリスティック関数適用後
勝った回数：18回
負けた回数：2回

もともと高い勝率だったためヒューリスティック関数の効果はあまり現れなかった。



\section{課題2-2a}

\begin{screen}
評価関数の仕様を答えなさい．
\end{screen}

\section{課題2-2b}
\begin{screen}
人がコンピュータと対戦できるようにプログラムを作成せよ
\end{screen}


\section{課題2-2c}
\begin{screen}
リスト 8 をネガマックスで実装せよ
\end{screen}

\section{課題2-2d}
\begin{screen}
Move Ordering の有無による訪問ノード数の変化を具体的に報告せよ
\end{screen}

\section{課題2-2e}
\section{課題2-1}
\begin{screen}
対称（反転・回転）な状態を重複とみなす場合とみなさない場合でそれぞれ総状態数を答えよ
\end{screen}

\section{課題2-2}
\begin{screen}
初期状態における先手の勝敗（必勝・必敗・引分け）を答えよ．また，先手が勝利確定となる局面数，敗北確定となる局面数，引き分けとなる局面数をそれぞれ答えよ
\end{screen}


\subsection{考察}

\paragraph{A*アルゴリズムの削減率について}
A*アルゴリズムでは、4つの初期状態すべてで削減率を計算することができた。削減率は60.00\から92.47\と非常に高く、特にExampleとn=200、n=300の問題では約90\以上の削減を達成している。n=100の問題では削減率が60.00\と比較的低いが、これは元々の問題が簡単で訪問ノード数が少なかったためと考えられる。

\paragraph{最良優先探索について}
最良優先探索では、繰り返し回避なしの場合、すべての初期状態で上限（50,000訪問ノード）に達してしまった。一方、繰り返し回避ありの場合は404ノード以下で解を発見できており、繰り返し回避の効果が極めて大きいことが確認できた。最良優先探索はヒューリスティック関数のみを評価するため、同じ状態を何度も訪問してしまう傾向が強いと考えられる。

\paragraph{最小コスト優先探索について}
最小コスト優先探索では、繰り返し回避なしの場合は処理が遅すぎるため計測を省略した。繰り返し回避ありの場合でも230から4,674ノードと比較的多くのノードを訪問しており、他の手法と比較して効率が悪い。これは経路コストのみを評価するため、ゴールへの方向性が考慮されないためである。

\paragraph{結論}
繰り返し回避の導入により、訪問ノード数を大幅に削減できることが確認された。特にA*アルゴリズムでは平均約83.85\の削減率を達成し、効率的な探索が可能となった。最良優先探索では削減率の正確な計算はできなかったが、50,000ノードから数百ノードへの劇的な削減が見られた。8パズルのような状態空間探索では、繰り返し回避が必須の最適化手法であると言える。

各パズルの具体的な解法手順は「8-puzzle-ex12d.txt」に出力した。手数は以下の通りである。

\begin{itemize}
\item Example: 14手
\item Random n=100: 21手
\item Random n=200: 17手
\item Random n=300: 20手
\item Difficult Example: 28手
\end{itemize}

すべてのヒューリスティック関数で同じ手数の最適解が得られており、A*アルゴリズムが正しく機能していることが確認できる。

\subsection{考察}

\paragraph{訪問ノード数の比較}
ヒューリスティック関数が強いほど訪問ノード数が劇的に減少している。Difficult Exampleでは、$h'_1$が153,442ノード、$h'_2$が80,807ノード（約47\減）、$h'_3$が4,515ノード（約97\減）を訪問している。これは、より正確な残り手数の推定により、無駄な探索が削減されたためである。

Random n=100の場合、$h'_1$から$h'_3$への改善により訪問ノード数が約98\（21,920→409）削減されており、ヒューリスティック関数の質が探索効率に極めて大きな影響を与えることが分かる。

\paragraph{オープンリストとクローズドリストの最大長}
強いヒューリスティック関数ほど、オープンリストとクローズドリストの最大サイズが小さくなっている。これはメモリ使用量の削減につながり、より大規模な問題にも対応可能になることを示唆している。

\paragraph{実行時間の比較}
実行時間も訪問ノード数に比例して改善されている。Difficult Exampleでは、$h'_1$が約15分（909秒）かかったのに対し、$h'_3$は約2秒で解を発見しており、約440倍の高速化が達成されている。

\paragraph{ヒューリスティック関数の選択指針}
実験結果から、マンハッタン距離（$h'_3$）が最も優れた性能を示すことが明らかになった。誤配置タイル数（$h'_2$）も実用的な性能を持つが、難しい問題では$h'_3$との性能差が顕著になる。$h'_1$は探索効率が著しく低く、実用的ではない。

一般に、ヒューリスティック関数は「許容的（真の距離を過大評価しない）」かつ「できるだけ真の距離に近い」ことが望ましい。マンハッタン距離はこの両方の性質を高いレベルで満たしているため、8パズルにおいて最適なヒューリスティック関数と言える。

\paragraph{大小関係の実験的検証}
すべての実験結果において、3つのヒューリスティック関数は同じ手数の最適解を発見している。これは、すべてのヒューリスティック関数が許容的（$h' \leq h$）であり、A*アルゴリズムの最適性が保証されていることを示している。

\end{document}

\section{AI利用記録}

利用目的 コードのデバックを行うため
利用段階 EightPuzzleProblem クラスの実装段階．
利用内容 System.out.printlnの内容や使用する変数の提案
検証方法 実行結果を授業資料の例と比較し，期待どおりの解が得られることを確認した．